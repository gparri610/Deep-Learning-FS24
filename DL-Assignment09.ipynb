{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-rBVqNINttd"
      },
      "source": [
        "### Group Members:\n",
        "\n",
        "- Giovanni Sergio Armido Parri, 16-919-144"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Assignment 9: Convolutional Auto-Encoder\n",
        "\n",
        "In this assignment, we show that it is possible to learn from unlabeled data using a convolutional auto-encoder network.\n",
        "The task is to reduce an image of the handwritten digits of MNIST into a deep feature representation, without making use of their labels, and reconstruct the sample from that representation.\n",
        "\n",
        "For this purpose, we implement a convolutional auto-encoder that learns a $K=10$-dimensional deep feature representation of each image and uses this representation to reconstruct images to the original size of $28\\times28$ pixels.\n",
        "We show that such a network can be used to detect anomalies in the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjNUIs9WRTgF"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "We will make use of `torchvision.datasets.MNIST` dataset and `torchvision.datasets.FashionMNIST` dataset.\n",
        "The former has 10 labels of digit images and the latter has 10 labels of merchandise images.\n",
        "However, besides the last task, we do not make use of the labels of the dataset, but we only utilize the images.\n",
        "\n",
        "These datasets can be split into train sets and test sets by the default implementation.\n",
        "We will use solely MNIST dataset for training and validation of autoencoder network.\n",
        "To use this network for anomaly detection, we will create a new testing set by combining the whole test set of MNIST and a\n",
        "randomly selected subset of FashionMNIST test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwujs3MtOUBV"
      },
      "source": [
        "### Task 1: Datasets\n",
        "Write a dataset class that derives from `torch.utils.data.Dataset` that returns three values: data, target, and anomaly label. Implement the following three functions:\n",
        "\n",
        "1. The constructor `__init__(self, root, purpose, transform, anomaly_size)`. `root` is the saving path of dataset, `purpose` should be `train`, `val`, or `anomaly_detection`, and `anomaly_size` decides the number of test samples from FashionMNIST dataset. `torchvision.datasets.MNIST` and `torchvision.datasets.FashionMNIST` should be appropriately called here depending on `purpose`. Particularly, when `purpose=anomaly_detection`, a subset of FashionMNIST dataset should be randomly selected with size `anomaly_size`.\n",
        "2. The function `__len__(self)` returns the number of samples in our dataset.\n",
        "3. The index function `__getitem__(self, idx)` returns image, target, and data type indicator (-1 if anomalous, 1 otherwise) for a given index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ztxUX_yhJPvv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "\n",
        "class MixedDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root='./data', purpose=\"train\", transform=None, anomaly_size=2000):\n",
        "        self.purpose = purpose\n",
        "        # load MNIST dataset based on \"purpose\"\n",
        "        if purpose == \"train\":\n",
        "            self.mnist_dataset = torchvision.datasets.MNIST(root=root, train=True, download=True, transform=transform)\n",
        "        elif purpose == \"val\":\n",
        "            self.mnist_dataset = torchvision.datasets.MNIST(root=root, train=False, download=True, transform=transform)\n",
        "        # load FashionMNIST dataset when \"purpose\" is \"anomaly_detection\" and randomly select samples with size \"anomaly_size\"\n",
        "        elif purpose == \"anomaly_detection\":\n",
        "            self.fashion_mnist_dataset = torchvision.datasets.FashionMNIST(root=root, train=True , download=True, transform=transform)\n",
        "            self.fashion_mnist_dataset = torch.utils.data.Subset(self.fashion_mnist_dataset, torch.randperm(len(self.fashion_mnist_dataset))[:anomaly_size])\n",
        "            self.mnist_dataset = torchvision.datasets.MNIST(root=root, train=False, download=True, transform=transform)\n",
        "            self.dataset_anomal = torch.utils.data.ConcatDataset([self.mnist_dataset, self.fashion_mnist_dataset])\n",
        "        else:\n",
        "            raise ValueError(\"Purpose must be either 'train', 'val', or 'anomaly_detection'\")\n",
        "    def __len__(self):\n",
        "        # return length of the desired dataset based on its purpose\n",
        "        if self.purpose == \"train\":\n",
        "            return len(self.mnist_dataset)\n",
        "        elif self.purpose == \"val\":\n",
        "            return len(self.mnist_dataset)\n",
        "        elif self.purpose == \"anomaly_detection\":\n",
        "            return len(self.fashion_mnist_dataset)\n",
        "        else:\n",
        "            raise ValueError(\"Purpose must be either 'train', 'val', or 'anomaly_detection'\")\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # perform appropriate actions on the data, target, and its data type indicator (return 1 for regular and -1 for anomalous)\n",
        "        if self.purpose == \"train\":\n",
        "            image, target = self.mnist_dataset[idx]\n",
        "            data_type = 1\n",
        "        elif self.purpose == \"val\":\n",
        "            image, target = self.mnist_dataset[idx]\n",
        "            data_type = 1\n",
        "        elif self.purpose == \"anomaly_detection\":\n",
        "            image, target = self.dataset_anomal[idx]\n",
        "            if idx < len(self.mnist_dataset):\n",
        "                data_type = 1\n",
        "            else:\n",
        "                data_type = -1\n",
        "        else:\n",
        "            raise ValueError(\"Purpose must be either 'train', 'val', or 'anomaly_detection'\")\n",
        "        return image, target, data_type\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Oc30hvxO4Ko"
      },
      "source": [
        "### Task 2: Data Loader\n",
        "\n",
        "Call the dataset class three times, use `torchvision.transforms.ToTensor()` as transform, with purpose = `train` (batch size = 32), `val` (batch size = 100), and `anomaly_detection` (batch size = 1000). The third dataset has `anomaly_size=2000`.\n",
        "Instantiate data loader for the three datasets.\n",
        "Remember to shuffle the third dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Dyn2F-8BNaNV"
      },
      "outputs": [],
      "source": [
        "# define transform\n",
        "transform = torchvision.transforms.ToTensor()\n",
        "\n",
        "# instantiate training dataset and data loader\n",
        "train_dataset = MixedDataset(purpose=\"train\", transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# instantiate validation dataset and data loader\n",
        "val_dataset = MixedDataset(purpose=\"val\", transform=transform)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=100, shuffle=False)\n",
        "\n",
        "# instantiate anomaly detection dataset and data loader\n",
        "anomaly_detection_dataset = MixedDataset(purpose=\"anomaly_detection\", transform=transform)\n",
        "anomaly_detection_loader = torch.utils.data.DataLoader(anomaly_detection_dataset, batch_size=1000, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVI7sASPA1d5"
      },
      "source": [
        "### Test 1: Data Check\n",
        "\n",
        "Load one batch of train set, validation set, and anomaly detection set.\n",
        "For each set, check that input size of each sample is (1, 28, 28), targets are between 0 and 9, and the data type for the first two sets is 1 and there are -1 in the last set.\n",
        "\n",
        "Plot 20 samples from validation loader and 20 samples from anomaly detection loader and check whether samples from FashionMNIST are correctly loaded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "YdzbysQrKQ69"
      },
      "outputs": [
        {
          "ename": "AssertionError",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-b2d2727ad5d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m60000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manomaly_detection_dataset\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m12000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mx_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: "
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1080x432 with 40 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "from matplotlib import pyplot\n",
        "\n",
        "pyplot.rcParams['image.cmap'] = 'gray'\n",
        "\n",
        "fig, axes = pyplot.subplots(nrows=4, ncols=10, figsize=(15, 6))\n",
        "\n",
        "x_train, t_train, l_train = next(iter(train_loader))\n",
        "x_val, t_val, l_val = next(iter(val_loader))\n",
        "x_ad, t_ad, l_ad = next(iter(anomaly_detection_loader))\n",
        "assert len(train_dataset) == 60000\n",
        "assert len(val_dataset) == 10000\n",
        "assert len(anomaly_detection_dataset) == 12000\n",
        "assert x_train[0].shape == (1, 28, 28)\n",
        "assert x_val[0].shape == (1, 28, 28)\n",
        "assert x_ad[0].shape == (1, 28, 28)\n",
        "assert 0 <= t_train.all() <= 9\n",
        "assert 0 <= t_val.all() <= 9\n",
        "assert 0 <= t_ad.all() <= 9\n",
        "assert (l_val == 1).all()\n",
        "assert (l_train == 1).all()\n",
        "assert -1 in l_ad\n",
        "\n",
        "index=0\n",
        "for i in range(2):\n",
        "    for j in range(10):\n",
        "        axes[i][j].imshow(x_val[index].squeeze())\n",
        "        axes[i][j].axis(\"off\")\n",
        "        axes[i+2][j].imshow(x_ad[index].squeeze())\n",
        "        axes[i+2][j].axis(\"off\")\n",
        "        index+=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ohrvuk9qLX8a"
      },
      "source": [
        "## Auto-Encoder Network\n",
        "\n",
        "The auto-encoder network is composed of two parts: the encoder that transforms the input image to a deep feature representation; and the decoder that produces an image from such a deep feature.\n",
        "\n",
        "For the encoder $\\mathcal E$, we will use a similar convolutional network topology as in the past assignments.\n",
        "An exception is that we perform our down-sampling via striding and not via pooling.\n",
        "After each convolution, we apply the ReLU activation.\n",
        "The output of the encoder is a $K=10$ dimensional deep feature representation.\n",
        "The complete encoder network topology can be found below in Topology 1(a).\n",
        "\n",
        "The decoder $\\mathcal D$ performs the inverse operations of the encoder.\n",
        "A fully-connected layer is used to increase the number of samples to the same size as the output of the flattening of the encoder.\n",
        "Then, the flattening needs to be undone by reshaping the vector into the correct dimensionality, followed by a ReLU activation.\n",
        "A fractionally-strided convolutional layer increases the intermediate representation by a factor of 2.\n",
        "Note that the fractionally-strided convolution is implemented in `torch.nn.ConvTranspose2d`, and the `stride` parameter should have the same value as for the encoder.\n",
        "Additionally, the `torch.nn.ConvTranspose2d` has a parameter `output_padding` which needs to be adapted to reach the correct output shape (see Test 2).\n",
        "After this layer, we perform another ReLU activation and another fractionally-strided convolution to arrive at the original input dimension.\n",
        "The complete decoder network topology can be found below in Topology 1(b).\n",
        "\n",
        "Finally, we combine the two sub-networks into one auto-encoder network.\n",
        "While there exist several possibilities for doing this, we will implement a third `torch.nn.Module` that contains an instance of the encoder and an instance of the decoder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDSQU7nvluAM"
      },
      "source": [
        "Topology 1: Network configurations of the (a) encoder and (b) decoder networks\n",
        "\n",
        "(a) Encoder Network\n",
        "\n",
        "*   2D convolutional layer with $Q_1$ output channels, kernel size $5\\times5$, **stride 2** and padding 2\n",
        "*   activation function ReLU\n",
        "*   2D convolutional layer with $Q_2$ output channels, kernel size $5\\times5$, **stride 2** and padding 2\n",
        "*   flatten layer to convert the convolution output into a vector\n",
        "*   activation function ReLU\n",
        "*   fully-connected layer with the correct number of inputs and $K$ outputs\n",
        "\n",
        "(b) Encoder Network\n",
        "\n",
        "*   fully-connected layer with $K$ inputs and the correct number of outputs\n",
        "*   activation function ReLU\n",
        "*   reshaping to convert the vector into a convolution input\n",
        "*   2D **fractionally-strided convolutional** layer with $Q_2$ input channels, kernel size $5\\times5$, stride 2 and padding 2\n",
        "*   activation function ReLU\n",
        "*   2D **fractionally-strided convolutional** layer with $Q_1$ input channels, kernel size $5\\times5$, stride 2 and padding 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4SddasaA8We"
      },
      "source": [
        "### Task 3: Encoder Network\n",
        "\n",
        "Implement the encoder network for given parameters $Q_1$, $Q_2$, and $K$ as given in Topology 1(a).\n",
        "Implement a network class that derives from `torch.nn.Module` and implement the `__init__` and the `forward` methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y04EYtUUOsB0"
      },
      "outputs": [],
      "source": [
        "class Encoder (torch.nn.Module):\n",
        "    def __init__(self, Q1, Q2, K):\n",
        "        # call base class constrcutor\n",
        "        super(Encoder,self).__init__()\n",
        "        # convolutional define layers\n",
        "        self.conv1 = torch.nn.Conv2d(in_channels=1, out_channels=Q1, kernel_size=5, stride=2, padding=2)\n",
        "        self.conv2 = torch.nn.Conv2d(in_channels=Q1, out_channels=Q2, kernel_size=5, stride=2, padding=2)\n",
        "        # activation functions will be re-used for the different stages\n",
        "        self.act = torch.nn.ReLU()\n",
        "        # define fully-connected layers\n",
        "        self.flatten = torch.nn.Flatten()\n",
        "        self.fc = torch.nn.Linear(Q2*7*7, K)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # get the deep feature representation\n",
        "        deep_feature = self.fc(self.act(self.flatten(self.conv2(self.act(self.conv1(x))))))\n",
        "        return deep_feature"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zG9vfdv1A-cj"
      },
      "source": [
        "### Task 4: Decoder Network\n",
        "\n",
        "Implement the decoder network for given parameters $Q_1$, $Q_2$, and $K$ as given in Topology 1(b).\n",
        "Implement a network class that derives from `torch.nn.Module` and implement the `__init__` and the `forward` methods.\n",
        "The output of the decoder network is supposed to have values in the range $[0,1]$, similar to the input values.\n",
        "We need to make sure that only these values can be achieved.\n",
        "Think of possible ways of doing that, and apply the way that seems most reasonable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sUibY33HOwGx"
      },
      "outputs": [],
      "source": [
        "class Decoder (torch.nn.Module):\n",
        "    def __init__(self, Q1, Q2, K):\n",
        "        # call base class constrcutor\n",
        "        super(Decoder,self).__init__()\n",
        "        # fully-connected layer\n",
        "        self.fc = torch.nn.Linear(K, Q2*7*7)\n",
        "        # convolutional layers\n",
        "        self.deconv1 = torch.nn.ConvTranspose2d(in_channels=Q2, out_channels=Q1, kernel_size=5, stride=2, padding=2, output_padding=1)\n",
        "        self.deconv2 = torch.nn.ConvTranspose2d(in_channels=Q1, out_channels=1, kernel_size=5, stride=2, padding=2, output_padding=1)\n",
        "        # activation function\n",
        "        self.act = torch.nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # reconstruct the output image\n",
        "        output = self.deconv2(self.act(self.deconv1(self.act(self.fc(x).view(-1, 32, 7, 7)))))\n",
        "        output = torch.sigmoid(output) # apply sigmoid to ensure the output is in the range [0, 1]\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1LIBsPnBAXR"
      },
      "source": [
        "### Task 5: Joint Auto-Encoder Network\n",
        "\n",
        "Implement the auto-encoder network by combining the encoder and the decoder.\n",
        "In the `__init__` function, instantiate an encoder from Task 3 and a decoder from Task 4.\n",
        "In `forward`, pass the input through the encoder and the decoder: $\\mathbf Y = \\mathcal D(\\mathcal E(\\mathbf X))$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xjaPdugsOzoX"
      },
      "outputs": [],
      "source": [
        "class AutoEncoder(torch.nn.Module):\n",
        "    def __init__(self, Q1, Q2, K):\n",
        "        super(AutoEncoder,self).__init__()\n",
        "        self.encoder = Encoder(Q1, Q2, K)\n",
        "        self.decoder = Decoder(Q1, Q2, K)\n",
        "\n",
        "    def forward(self,x):\n",
        "        # encode input\n",
        "        deep_feature = self.encoder(x)\n",
        "        # decode to output\n",
        "        reconstructed = self.decoder(deep_feature)\n",
        "        return reconstructed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gc405D2xBDbm"
      },
      "source": [
        "### Test 2: Output Sizes\n",
        "\n",
        "The code below instantiates the auto-encoder network with $Q_1 = Q_2 = 32$ and $K=10$.\n",
        "Then the given input $\\mathbf X$ is provided to the (untrained) auto-encoder network.\n",
        "Use these codes to verify that the deep feature extracted by the encoder and the output from the decoder part both have the desired size. Also, verify that the output values are between 0 and 1.\n",
        "\n",
        "If the tests cannot be passed, please check the implementation in Task 3, 4, and 5."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jGdY4_xI9ANT"
      },
      "outputs": [],
      "source": [
        "# run on cuda device?\n",
        "device = torch.device(\"cpu\")#torch.device(\"cuda\")\n",
        "\n",
        "# create network\n",
        "network = AutoEncoder(32, 32, 10).to(device)\n",
        "\n",
        "# create or select a sample\n",
        "x = torch.randn((1,1,28,28))\n",
        "\n",
        "# use encoder to encode image and check its size\n",
        "deep_features = network.encoder(x.to(device))\n",
        "assert deep_features.shape[1] == 10\n",
        "\n",
        "# use decoder to generate an image and check its size and value range\n",
        "output = network.decoder(deep_features)\n",
        "assert output.shape[2:] == (28,28)\n",
        "assert torch.all(output >= 0) and torch.all(output <= 1) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyHtPO8tL5vI"
      },
      "source": [
        "## Training and Evaluation\n",
        "We will implement a training procedure for an auto-encoder network.\n",
        "\n",
        "To train the network, we will use the $L_2$ distance between the output and the input of the network as a loss function, which is implemented in `torch.nn.MSELoss`:\n",
        "\n",
        "  $$\\mathcal J^{L_2} (\\mathbf X, \\mathbf Y) = \\|\\mathbf X - \\mathbf Y\\|^2$$\n",
        "\n",
        "For optimization, we will make use of the `Adam` optimizer with a learning rate of $\\eta=0.001$.\n",
        "We will run the training for 10 epochs and compute training and validation set loss after each epoch.\n",
        "\n",
        "For evaluation, we will check whether some of the validation set samples are correctly reconstructed from the auto-encoder network by visualizing them in Task 9.\n",
        "\n",
        "In our defined anomaly detection validation set, there are 10000 regular samples and 2000 anomalous samples.\n",
        "In the case of evaluating the success rate of anomaly detection (Task 7 & 8), accuracy is not the best metric because it gives equal weights to two classes with unequal numbers of samples, while the actual distribution of the two classes is unbalanced.\n",
        "A system can achieve high accuracy by simply predicting the majority class for every instance, while completely ignoring the minority class.\n",
        "\n",
        "In such cases, we choose to use the true positive rate (TPR) and true negative rate (TNR), which measure the proportion of actual positive/negative cases that are correctly identified by the system.\n",
        "\n",
        "$$\\text{TPR} = \\frac{\\text{True Positives}}{\\text{True Positives+False Negatives}} = \\frac{\\text{True Positives}}{\\text{Positives}}$$\n",
        "\n",
        "$$\\text{TNR} = \\frac{\\text{True Negatives}}{\\text{True Negatives+False Positives}} = \\frac{\\text{True Negatives}}{\\text{Negatives}}$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ph5oHS3jBL03"
      },
      "source": [
        "### Task 6: Training Loop\n",
        "\n",
        "Instantiate the auto-encoder network with $Q_1 = Q_2 = 32$ and $K=10$.\n",
        "\n",
        "To train the auto-encoder network, we will use the $L_2$ distance between the output and the input of the network as a loss function.\n",
        "This loss function is implemented in `torch.nn.MSELoss`.\n",
        "\n",
        "Since training an auto-encoder is tricky, we will make use of the Adam optimizer.\n",
        "Choose a learning rate of $\\eta=0.001$. Implement the training loop for 10 epochs.\n",
        "\n",
        "Compute the average training loss and validation loss and print them at the end of each epoch. Note: If the training and validation loss does not decrease during training, try to reduce the learning rate (to $\\eta=0.0005$ or even lower) and re-start the training (remember to re-initialize the network, too).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TxaFzi4sZMaE"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1; train: 0.00108, val: 0.00021\n",
            "Epoch 2; train: 0.00058, val: 0.00018\n",
            "Epoch 3; train: 0.00052, val: 0.00017\n",
            "Epoch 4; train: 0.00049, val: 0.00016\n",
            "Epoch 5; train: 0.00047, val: 0.00015\n",
            "Epoch 6; train: 0.00045, val: 0.00015\n",
            "Epoch 7; train: 0.00044, val: 0.00015\n",
            "Epoch 8; train: 0.00043, val: 0.00014\n",
            "Epoch 9; train: 0.00042, val: 0.00014\n",
            "Epoch 10; train: 0.00041, val: 0.00014\n"
          ]
        }
      ],
      "source": [
        "# set device if available\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "# create network\n",
        "network = AutoEncoder(32, 32, 10).to(device)\n",
        "\n",
        "# define optimizer\n",
        "optimizer = torch.optim.Adam(network.parameters(), lr=0.0005)\n",
        "\n",
        "# define loss function\n",
        "loss = torch.nn.MSELoss()\n",
        "\n",
        "for epoch in range(2): \n",
        "    # evaluate average loss for training and validation set per epoch\n",
        "    train_loss = validation_loss = 0.\n",
        "\n",
        "    for x,_,_ in train_loader:\n",
        "\n",
        "        # compute netwok output\n",
        "        y = network(x.to(device))\n",
        "        # compute loss between output and input\n",
        "        J = loss(y, x.to(device))\n",
        "\n",
        "        # perform update\n",
        "        optimizer.zero_grad()\n",
        "        J.backward()\n",
        "        optimizer.step()\n",
        "        # accumulate loss\n",
        "        train_loss += J.item()\n",
        "\n",
        "\n",
        "    # compute validation loss\n",
        "    with torch.no_grad():\n",
        "        for x,t,_ in val_loader:\n",
        "            # compute network output\n",
        "            y = network(x.to(device))\n",
        "            # compute loss\n",
        "            J = loss(y, x.to(device))\n",
        "            # accumulate loss\n",
        "            validation_loss += J.item()\n",
        "\n",
        "\n",
        "\n",
        "    # print average loss for training and validation\n",
        "    print(f\"\\rEpoch {epoch+1}; train: {train_loss/len(train_dataset):1.5f}, val: {validation_loss/len(val_dataset):1.5f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXFrw0vjNYjs"
      },
      "source": [
        "### Task 7: True Positive/Negative Rate Calculation\n",
        "\n",
        "Define a function that takes the predictions and binary ground-truth values (the data type from Task 1) as lists, and returns TPR and TNR.\n",
        "\n",
        "You can use `sklearn.metrics.confusion_matrix` (https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) to compute true positives, true negatives, false positives, and false negatives, or compute them by their definitions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LfKZRU5-N-K5"
      },
      "outputs": [],
      "source": [
        "import sklearn.metrics \n",
        "import numpy as np\n",
        "\n",
        "def compute_tpr_tnr(predictions, truth):\n",
        "    # convert list into numpy arrary\n",
        "    predictions = np.array(predictions)\n",
        "    truth = np.array(truth)\n",
        "\n",
        "    # Compute the confusion matrix or tp, tn, fp, fn\n",
        "    conf_matrix = sklearn.metrics.confusion_matrix(truth, predictions)\n",
        "    tn, fp, fn, tp = conf_matrix.ravel()\n",
        "    # Compute TPR and TNR\n",
        "    tpr = tp/(tp+fn)\n",
        "    tnr = tn/(tn+fp)\n",
        "\n",
        "\n",
        "    return tpr, tnr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYQTLLvjOCJF"
      },
      "source": [
        "### Test 3: TPR & TNR Calculation Check\n",
        "\n",
        "With the given truth values and predictions, call the function defined in Task 7 and check the returned TPR and TNR are correct."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N_JYYlV6IMkt"
      },
      "outputs": [],
      "source": [
        "truth = [1, 1, 1, 1, 1, 1, -1, 1, 1, -1, -1, 1, 1, -1, 1, -1, 1, 1, 1, 1]\n",
        "predictions = [1, -1, 1, 1, 1, 1, -1, 1, 1, -1, 1, 1, 1, -1, 1, -1, 1, 1, 1, 1]\n",
        "\n",
        "tpr, tnr = compute_tpr_tnr(predictions, truth)\n",
        "assert(abs(tpr - 14/15) < 1e-8)\n",
        "assert(abs(tnr - 0.8) < 1e-8)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJBd6La0DllW"
      },
      "source": [
        "### Task 8: Anomaly Evaluation\n",
        "\n",
        "In this task, forward the data from the anomaly detection loader into the trained network, compute sample-wise loss (https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html), and assign -1 to samples with loss > 0.04, and 1 to samples with loss < 0.04.\n",
        "\n",
        "Forward the re-assigned loss as predictions into the function defined in Task 7 to compute TPR and TNR. Compute the regular accuracy as well to make a comparison."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VhJjWfUG_zPs"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True Positive Rate: nan\n",
            "True Negative Rate: 0.16125\n",
            "Accuracy: 0.16125\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-9-e1ed3eedabca>:13: RuntimeWarning: invalid value encountered in long_scalars\n",
            "  tpr = tp/(tp+fn)\n"
          ]
        }
      ],
      "source": [
        "# instantiate loss function with appropriate reduction\n",
        "loss = torch.nn.MSELoss(reduction='none')\n",
        "\n",
        "correct = 0.\n",
        "predictions = []\n",
        "truth_values = []\n",
        "\n",
        "# compute tpr and tnr for the anomaly detection dataset\n",
        "with torch.no_grad():\n",
        "    for x, t, l in anomaly_detection_loader:\n",
        "        # forward input to the trained network\n",
        "        y = network(x.to(device))\n",
        "        # compute loss per input\n",
        "        J = loss(y, x.to(device)).mean(dim=[1,2,3])\n",
        "        # select the indexes at which loss is great then threshold 0.04 and replace its value by -1, and others by 1.\n",
        "        prediction = torch.where(J > 0.04, -1, 1)\n",
        "        # convert the prediction and l into list and add to list predictions and list truth_values\n",
        "        predictions.append(prediction.tolist())\n",
        "        truth_values.append(l.tolist())\n",
        "        # compute accuracy\n",
        "        correct += (prediction == l).sum().item()\n",
        "\n",
        "# print the accuracy\n",
        "acc = correct/len(anomaly_detection_dataset)\n",
        "# compute tpr and tnr with function defined in Task 7 and print tpr and tnr\n",
        "predictions = [item for sublist in predictions for item in sublist]\n",
        "truth_values = [item for sublist in truth_values for item in sublist]\n",
        "tpr, tnr = compute_tpr_tnr(predictions, truth_values)\n",
        "\n",
        "print(\"True Positive Rate:\", tpr)\n",
        "print(\"True Negative Rate:\", tnr)\n",
        "print(\"Accuracy:\", acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jO5VTYaaBRx2"
      },
      "source": [
        "### Task 9: Reconstruction Result\n",
        "\n",
        "This task is to visualize the reconstructed images from their originals.\n",
        "For this purpose, load the first batch of the anomaly detection set. \n",
        "For both MNIST and FashionMNIST data, select one image for each label.\n",
        "\n",
        "Forward the images through the trained auto-encoder network to extract their reconstructions.\n",
        "\n",
        "Make a single plot with 4 rows and 10 columns. \n",
        "In the 1st/3rd row, plot the original MNIST/FashionMNIST samples, 2nd/4th row plot the corresponding reconstructed samples. \n",
        "See the reference plot in the slides."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJANkSFe-DsC"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Number of rows must be a positive integer, not Ellipsis",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-479ee8bbd457>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mpyplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrcParams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image.cmap'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'gray'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mncols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.8/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36msubplots\u001b[0;34m(nrows, ncols, sharex, sharey, squeeze, width_ratios, height_ratios, subplot_kw, gridspec_kw, **fig_kw)\u001b[0m\n\u001b[1;32m   1441\u001b[0m     \"\"\"\n\u001b[1;32m   1442\u001b[0m     \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mfig_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m     axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,\n\u001b[0m\u001b[1;32m   1444\u001b[0m                        \u001b[0msqueeze\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubplot_kw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubplot_kw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m                        \u001b[0mgridspec_kw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgridspec_kw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight_ratios\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheight_ratios\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.8/site-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36msubplots\u001b[0;34m(self, nrows, ncols, sharex, sharey, squeeze, width_ratios, height_ratios, subplot_kw, gridspec_kw)\u001b[0m\n\u001b[1;32m    891\u001b[0m             \u001b[0mgridspec_kw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'width_ratios'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwidth_ratios\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 893\u001b[0;31m         \u001b[0mgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_gridspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mncols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mgridspec_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    894\u001b[0m         axs = gs.subplots(sharex=sharex, sharey=sharey, squeeze=squeeze,\n\u001b[1;32m    895\u001b[0m                           subplot_kw=subplot_kw)\n",
            "\u001b[0;32m~/.local/lib/python3.8/site-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36madd_gridspec\u001b[0;34m(self, nrows, ncols, **kwargs)\u001b[0m\n\u001b[1;32m   1513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1514\u001b[0m         \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'figure'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pop in case user has added this...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1515\u001b[0;31m         \u001b[0mgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSpec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mncols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mncols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1516\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.8/site-packages/matplotlib/gridspec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, nrows, ncols, figure, left, bottom, right, top, wspace, hspace, width_ratios, height_ratios)\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfigure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m         super().__init__(nrows, ncols,\n\u001b[0m\u001b[1;32m    389\u001b[0m                          \u001b[0mwidth_ratios\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwidth_ratios\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m                          height_ratios=height_ratios)\n",
            "\u001b[0;32m~/.local/lib/python3.8/site-packages/matplotlib/gridspec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, nrows, ncols, height_ratios, width_ratios)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \"\"\"\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIntegral\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mnrows\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m     50\u001b[0m                 f\"Number of rows must be a positive integer, not {nrows!r}\")\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mncols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIntegral\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mncols\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Number of rows must be a positive integer, not Ellipsis"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 1080x432 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# get the first anomaly detection set batch\n",
        "x, t, l = next(iter(anomaly_detection_loader))\n",
        "# select one image for each label\n",
        "...\n",
        "\n",
        "# If required, convert the list of select images into a tensor through torch.stack\n",
        "original_mnist = ...\n",
        "original_fashionmnist = ...\n",
        "\n",
        "# Generate reconstructed samples\n",
        "generated_mnist = ...\n",
        "generated_fashionmnist = ...\n",
        "\n",
        "samples = [original_mnist, generated_mnist, original_fashionmnist, generated_fashionmnist]\n",
        "\n",
        "# plot images\n",
        "pyplot.rcParams['image.cmap'] = 'gray'\n",
        "\n",
        "fig, axes = pyplot.subplots(nrows=..., ncols=..., figsize=(15, 6))\n",
        "\n",
        "for i in range(...):\n",
        "    for j in range(...):\n",
        "        axes[i][j].imshow(...)\n",
        "        axes[i][j].axis(\"off\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
