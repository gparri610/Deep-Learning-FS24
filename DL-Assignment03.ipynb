{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group Members:\n",
    "\n",
    "- Giovanni Sergio Armido Parri, 16-919-144"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3: Universal Function Approximator\n",
    "\n",
    "\n",
    "The goal of this exercise is to train a two-layer fully-connected network to perform one-dimensional non-linear regression via gradient descent. To show the flexibility of the approach, three different functions will be approximated. First, the network and its gradient need to be implemented. Second, target data for three different functions will be generated. Finally, the training procedure will be applied to the data, and the resulting approximated function will be plotted together with the data samples.\n",
    "\n",
    "## Network Implementation\n",
    "\n",
    "A two-layer network is defined by parameters $\\Theta=(\\mathbf W^{(1)},\\vec w^{(2)})$ that are split into $\\mathbf W^{(1)}\\in\\mathbb R^{K\\times {(D+1)}}$ for the first layer and $\\vec w^{(2)}\\in\\mathbb R^{K+1}$ for the second layer. In our case, since we have only a single input, we have $D=1$.\n",
    "For a given input $\\vec x = (1, x)^T$, the network is implemented in three steps:\n",
    "\n",
    "1. Compute the first layer output, aka, the activation: $\\vec a_- = \\mathbf W^{(1)} \\vec x$\n",
    "2. Apply the activation function for each element of $\\vec a_- : \\vec h_- = g(\\vec a_-)$ and prepend the bias neuron $h_0=1$ to arrive at $\\vec h$.\n",
    "3. Compute the output of the network: $y = \\vec w^{(2)}\\ ^T\\vec h$.\n",
    "\n",
    "### Task 1  \n",
    "Implement a function that returns the network output for a given input $\\vec x$ and parameters $\\Theta=(\\mathbf W^{(1)}, \\vec w^{(2)})$. Remember that the input of the function $\\vec x = (1, x)^T$. Also remember to prepend $h_0=1$ in your implementation.\n",
    "\n",
    "We use sigmoid $(\\sigma)$ as the activation function:\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\sigma(a) =  \\frac{\\mathrm{1} }{\\mathrm{1} + e^{-a} }\n",
    "\\end{equation*}\n",
    "\n",
    "Note:\n",
    "\n",
    "1. Use the `numpy` to implement the sigmoid function.\n",
    "2. Use `numpy.concatenate` or `numpy.insert` to prepend $h_0$.\n",
    "3. Make use of `numpy.dot` to compute matrix-vector and vector-vector products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def logistic(x):\n",
    "  return 1./(1.+np.exp(-x))\n",
    "\n",
    "def network(x, Theta):\n",
    "  W1, w2 = Theta\n",
    "  a_ = np.dot(W1,x)\n",
    "  h_ = logistic(a_)\n",
    "  h = np.insert(h_, 0,1.)\n",
    "  y = np.dot(w2, h)\n",
    "  return y, h"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test 1: Sanity Check\n",
    "----------------------------\n",
    "\n",
    "We select a specific number of hidden neurons and create the weights accordingly, using all zeros in the first layer and all ones in the second. The test case below assures that the function from Task 1 actually returns $11$ for those weights.\n",
    "\n",
    "Note: your function should pass the test below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Test passed\n"
     ]
    }
   ],
   "source": [
    "K = 20\n",
    "D = 1\n",
    "W1 = np.zeros((K, D+1))\n",
    "w2 = np.ones(K+1)\n",
    "print(w2)\n",
    "x = np.random.rand(D+1)\n",
    "y,h = network(x, (W1, w2))\n",
    "assert abs(11 - y) < 1e-6\n",
    "print(\"Test passed\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Implementation\n",
    "\n",
    "In order to perform gradient descent, we need to define a loss function. As provided in the lecture, the $\\mathcal J^{L_2}$ loss function is defined over a dataset $X=\\{(\\vec x^{[n]}, t^{[n]})\\}$, that is defined as a list of tuples, as follows:\n",
    "\n",
    "$$\n",
    "   \\mathcal J^{L_2} = \\frac{1}{N}\\sum_{i=1}^N (y^{[n]}-t^{[n]})^2\n",
    "$$\n",
    "\n",
    "where $y^{[n]}$ is the output of the network from Task 1 when inputting $\\vec x^{[n]}$. Interestingly, however, we never explicitly need to compute the output of the loss function. It is only used to analytically compute the gradient as shown in the lecture.\n",
    "\n",
    "The gradient is composed of two items, one for each layer. Particularly, for a given dataset $X$, the gradient of loss $J^{L_2}$ is defined as:\n",
    "\n",
    "\\begin{align}\n",
    "  \\frac{\\partial \\mathcal J}{\\partial w_{kd}^{(1)}} &= \\frac{2}{N} \\sum\\limits_{n=1}^N (y^{[n]}-t^{[n]}) w_{k}^{(2)} (1-h_{k}^{[n]}) h_{k}^{[n]} x_{d}^{[n]}\\\\\n",
    "  \\frac{\\partial \\mathcal J}{\\partial w_{k}^{(2)}} &= \\frac{2}{N} \\sum\\limits_{n=1}^N (y^{[n]}-t^{[n]}) h_{k}^{[n]}\n",
    "\\end{align}\n",
    "\n",
    "### Task 2\n",
    "Implement a function that returns the gradient as defined in $(1)$ and $(2)$ for a given dataset $X$, and given weights $\\Theta=(\\mathbf W^{(1)},\\vec w^{(2)})$. Make sure that both parts of the gradient are computed. \n",
    "\n",
    "Hint:\n",
    "\n",
    "1. Make use of the the function implemented in Task 1 where appropriate\n",
    "\n",
    "Note:\n",
    "\n",
    "  1. This is a slow implementation. We will see how to speed this up in the next lecture.\n",
    "  2. You can make use of `numpy.zeros` to initialize the gradient.\n",
    "  3. The outer product can be computed via `numpy.outer`.\n",
    "  4. Remember that we used the $logistic$ activation function in our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(X, Theta):\n",
    "  # split parameters for easier handling\n",
    "  W1, w2 = Theta\n",
    "\n",
    "  # define gradient with respect to both parameters\n",
    "  dW1 = np.zeros_like(W1)\n",
    "  dw2 = np.zeros_like(w2)\n",
    "  print(dW1.shape)\n",
    "  print(dw2.shape)\n",
    "  # iterate over dataset\n",
    "  for x, t in X:\n",
    "    # compute the gradient\n",
    "    y, h = network(x, Theta)\n",
    "\n",
    "    dw2 += 2./len(X)*np.dot((y-t), h)\n",
    "\n",
    "\n",
    "\n",
    "    for k in range(dW1.shape[0]):\n",
    "      dW1[:,k]= 2./len(X)*np.outer((y-t)[k]*w2[k]*(1 - h[k])*h[k], x)\n",
    "\n",
    "  # anything else?\n",
    "  ...\n",
    "\n",
    "  return dW1, dw2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "The procedure of gradient descent is the repeated application of two steps.\n",
    " \n",
    "1. The gradient of loss $\\nabla_{\\Theta}\\mathcal J^{L_2}$ is computed based on the current value of the parameters $\\Theta=(\\mathbf W^{(1)},\\vec w^{(2)})$.\n",
    "2. The weights are updated by moving a small step $\\eta$ into the direction of the negative gradient:\n",
    "\n",
    "$$\n",
    "    \\Theta = \\Theta - \\eta \\nabla_{\\Theta}\\mathcal J\n",
    "$$\n",
    "\n",
    "As stopping criterion, we select the number of training epochs to be 10000.\n",
    "\n",
    "### Task 3\n",
    "Implement a function that performs gradient descent for a given dataset $X$, given initial parameters $\\Theta$ and a given learning rate $\\eta$ and returns the optimized parameters $\\Theta^*$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, Theta, eta):\n",
    "  epochs = 10000\n",
    "  Theta_opt = np.copy(Theta)\n",
    "  OPT = [Theta_opt]\n",
    "  # perform iterative gradient descent\n",
    "  for epoch in range(epochs):\n",
    "    # compute the gradient\n",
    "    grad = gradient(X, OPT[epoch])\n",
    "\n",
    "    # update the parameters\n",
    "    OPT.append(OPT[epoch]-eta*grad)\n",
    "\n",
    "  # return optimized parameters\n",
    "  return OPT[-1]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Datasets\n",
    "\n",
    "In total, we will test our gradient descent function with three different datasets. Particularly, we approximate:\n",
    "\n",
    "1. $X_1: t = \\sin(3x)$ for $x\\in[-1,1]$\n",
    "2. $X_2: t = e^{-4x^2}$ for $x\\in[-1,1]$\n",
    "3. $X_3: t = x^5 + 3x^4 - 6x^3 -12x^2 + 5x + 129$ for $x\\in[-4,2.5]$\n",
    "\n",
    "### Task 4\n",
    "\n",
    "Generate dataset $X_1$, for $N=60$ samples randomly drawn from range $x\\in[-1,1]$. \n",
    "Generate data $X_2$ for $N=50$ samples randomly drawn from range $x\\in[-1,1]$. \n",
    "Generate dataset $X_3$ for $N=200$ samples randomly drawn from range $x\\in[-4,2.5]$. \n",
    "Implement all three datasets as lists of tuples: $\\{(\\vec x^{[n]}, t^{[n]})\\mid 1\\leq n\\leq N\\}$.\n",
    "\n",
    "Note:\n",
    "\n",
    "  1. You can use `numpy.random.uniform` to create uniformly distributed samples for $x$.\n",
    "  2. Make sure that $\\vec x = (1, x)^T$ for each sample.\n",
    "  3. You can make use of `numpy.sin`, `numpy.exp` and `numpy.pow` to compute target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#set random numbers to be the same\n",
    "np.random.seed(42)\n",
    "\n",
    "# X1\n",
    "x_1 = np.random.uniform(-1, 1, 60)\n",
    "t_1 = np.sin(3*x_1)\n",
    "x_1_bias = np.vstack((np.ones(60), x_1)).T\n",
    "\n",
    "X1 = [(x_1_bias[i], t_1[i]) for i in range(60)]\n",
    "\n",
    "\n",
    "# X2\n",
    "x_2 =np.random.uniform(-1, 1, 50)\n",
    "t_2 = np.exp(-4.*x_2)\n",
    "x_2_bias  = np.vstack((np.ones(50), x_2)).T\n",
    "\n",
    "X2 = [(x_2_bias[i], t_2[i]) for i in range(50)]\n",
    "\n",
    "# X3\n",
    "x_3 = np.random.uniform(-4, 2.5, 200)\n",
    "t_3 = np.power(x_3, 5) + 3.*np.power(x_3, 4) -6.*np.power(x_3, 3)-12.*np.power(x_3, 2)+5.*x_3+129\n",
    "x_3_bias = np.vstack((np.ones(200), x_3)).T\n",
    "\n",
    "X3 = [(x_3_bias[i], t_3[i]) for i in range(200)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2: Sanity Check\n",
    "\n",
    "The test case below assures that the elements of each generated dataset are tuples with two elements, that the first element ($\\vec x$) is a vector with two numbers, and that the second element ($t$) is a float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "assert all(\n",
    "    isinstance(x, (tuple,list)) and\n",
    "    len(x) == 2 and\n",
    "    isinstance(x[0], (tuple,list,np.ndarray)) and\n",
    "    len(x[0]) == 2 and\n",
    "    isinstance(x[1], float)\n",
    "    for X in (X1, X2, X3)\n",
    "    for x in X\n",
    ")\n",
    "\n",
    "print('Test passed!')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Approximation\n",
    "\n",
    "Finally, we want to make use of our gradient descent implementation to approximate our functions. In order to see our success, we want to plot the functions together with the data.\n",
    "\n",
    "### Task 5 (theoretical question)\n",
    "\n",
    "When looking at the example plots in the exercise slides (exemplary solutions for the plotting Task 8), how many hidden neurons $K$ do we need in order to approximate the functions? Is there any difference between the three target functions? **Please discuss it in the markdown cell below:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of neurons for each target function based on your discussion\n",
    "K1 = 2\n",
    "K2 = 3\n",
    "K3 = 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6\n",
    "\n",
    "For each of the datasets, randomly initialize the parameters $\\Theta_1,\\Theta_2,\\Theta_3\\in[-1,1]$ according to the number of hidden neurons estimated in Task 5.\n",
    "\n",
    "Note:\n",
    "\n",
    "  1. You can use `numpy.random.uniform` to initialize the weights.\n",
    "  2. Make sure that the weight matrices are instantiated in the correct dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Theta1 = (np.random.uniform(-1,1, (K1, D+1)), np.random.uniform(-1,1, K1+1))\n",
    "Theta2 = (np.random.uniform(-1,1, (K2, D+1)), np.random.uniform(-1,1, K2+1))\n",
    "Theta3 = (np.random.uniform(-1,1, (K3, D+1)), np.random.uniform(-1,1, K3+1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7\n",
    "\n",
    "Call gradient descent function from Task 3 using the datasets $X_1, X_2, X_3$, the according created parameters $\\Theta_1,\\Theta_2,\\Theta_3$. Store the resulting optimized weights $\\Theta_1^*, \\Theta_2^*, \\Theta_3^*$ and the loss values.\n",
    "\n",
    "**Please discuss first the appropriate learning rate $\\eta$  for each of the three functions in the markdown cell below.** Based on your learning rates, you may neeed to optimize the learning rate $\\eta$ for these functions. Do you see any differences? What are the best learning rates that you can find?\n",
    "\n",
    "WARNING: Depending on the implementation, this might run for several minutes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2)\n",
      "(3,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/giovanniparri/.local/lib/python3.8/site-packages/numpy/lib/function_base.py:959: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, order=order, subok=subok, copy=True)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-58614c727602>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mTheta1_opt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradient_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTheta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mTheta2_opt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradient_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTheta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mTheta3_opt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradient_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTheta3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-052b5734d6db>\u001b[0m in \u001b[0;36mgradient_descent\u001b[0;34m(X, Theta, eta)\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# compute the gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOPT\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# update the parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-55b9bb1ec068>\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(X, Theta)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdW1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m       \u001b[0mdW1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m2.\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mouter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mw2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0;31m# anything else?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "# Call gradient descent function using the datasets and initial weights that you created above\n",
    "# Choose appropriate learning rates for each function\n",
    "\n",
    "eta1 = 1e-5\n",
    "eta2 = 1e-5\n",
    "eta3 = 1e-5\n",
    "\n",
    "\n",
    "Theta1_opt = gradient_descent(X1, Theta1, eta1)\n",
    "Theta2_opt = gradient_descent(X2, Theta2, eta2)\n",
    "Theta3_opt = gradient_descent(X3, Theta3, eta3)\n",
    "\n",
    "print(Theta1_opt)\n",
    "print(Theta2_opt)\n",
    "print(Theta3_opt)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data and Function Plotting\n",
    "\n",
    "### Task 8\n",
    "\n",
    "Implement a plotting function that takes a given dataset $X$, given parameters $\\Theta$ and a defined range $R=[min,max]$. Each data sample $(x^{[n]},t^{[n]})$ of the dataset is plotted as an $''x''$. In order to plot the function that is approximated by the network, generate sufficient equally-spaced input values $x\\in R$, compute the network output $y$ for these inputs, and plot them with a line.\n",
    "\n",
    "Note:\n",
    "\n",
    "  1. The dataset $X$ is defined as above, a list of tuples $(\\vec x, t)$.\n",
    "  2. Each input in the dataset is defined as $\\vec x = (1,x)^T$.\n",
    "  3. Equidistant points can be obtained via `numpy.arange`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "def plot(X, Theta, R):\n",
    "  # first, plot data samples\n",
    "  pyplot.plot(..., ..., \"rx\", label=\"Data\")\n",
    "\n",
    "  # define equidistant points from min (R[0]) to max (R[1]) to evaluate the network\n",
    "  x = ...\n",
    "  # compute the network outputs for these values\n",
    "  y = ...\n",
    "  # plot network approximation\n",
    "  pyplot.plot(x,y,\"k-\", label=\"network\")\n",
    "  pyplot.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 9\n",
    "\n",
    "For each of the datasets and their according optimized parameters, call the plotting function from Task 8. Use range $R=[-1.5,1.5]$ for dataset $X_1$ and $X_2$, and range $R=[-5,4]$ for dataset $X_3$. Note that the first element of range $R$ should be the lowest $x$-location, and the second element of $R$ the highest value for $x$. Did the networks approximate the functions? What can we do if not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = pyplot.figure(figsize=(10,3))\n",
    "\n",
    "# plot first function\n",
    "pyplot.subplot(131)\n",
    "...\n",
    "\n",
    "# plot second function\n",
    "pyplot.subplot(132)\n",
    "...\n",
    "\n",
    "# plot third function\n",
    "pyplot.subplot(133)\n",
    "..."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2dd53f8ad749bca69f7250ce75eb4f0def59db5cf79075a9716322ffc58e8a2e"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
