{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjS_p66DM6_O"
      },
      "source": [
        "### Group Members:\n",
        "\n",
        "- Giovanni Sergio Armido Parri, 16-919-144"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsSWtTLDm0Lx"
      },
      "source": [
        "# Assignment 8: Open-Set Classification\n",
        "\n",
        "In this assignment, we develop a network that is capable of correctly classifying known classes while at the same time rejecting unknown samples that occur during inference time.\n",
        "To showcase the capability, we make use of the MNIST dataset that we artificially split into known and unknown classes; this allows us to train a network on the data without requiring too expensive hardware.\n",
        "\n",
        "\n",
        "We select the MNIST dataset and define several classes to be known, negative class during training, and unknown (not used for training at all).\n",
        "\n",
        "## Dataset\n",
        "We split the MNIST dataset into 4 known classes, 4 negative classes (used for training) and 2 unknown classes (used only for testing).\n",
        "While several splits might be possible, here we restrict to the following:\n",
        "* Known class indexes: (1, 4, 5, 8)\n",
        "* negative class indexes: (0, 2, 3, 7)\n",
        "* Unknown class indexes: (6,9)\n",
        "\n",
        "Please note that, in PyTorch, class indexing starts at 0 (other than in the lecture where class indexing starts at 1).\n",
        "\n",
        "We rely on the `torchvision.datasets.MNIST` implementation of the `MNIST` dataset, which we adapt to our needs.\n",
        "The constructor of our Dataset class takes one parameter that defines the purpose of this dataset (`\"train\", \"validation\", \"test\"`).\n",
        "The `\"train\"` partition uses the training samples of the *known* and the *negative* classes.\n",
        "The `\"validation\"` partition uses the test samples of the *known* and the *negative* classes.\n",
        "Finally, the `\"test\"` partition uses the test samples of the *known* and the *unknown* classes.\n",
        "\n",
        "In our implementation of the Dataset class, we need to implement two functions.\n",
        "* First, the constructor `__init__(self, purpose)` selects the data based on our purpose.\n",
        "* Second, the index function `__getitem__(self, n)` returns a pair $(X^n, \\vec t^n)$ for the sample with the index $n$, where $X \\in \\mathbb R^{1\\times28\\times28}$ with values in range $[0,1]$ and $\\vec t \\in \\mathbb R^{O}$, see below.\n",
        "\n",
        "Since our loss function (cf. Task 5) requires our target vectors to be in vector format, we need to convert the target index $t^n$ into its vector representation $\\vec t^n$.\n",
        "Particularly, we need to provide the following target vectors:\n",
        "\n",
        "<center>\n",
        "\n",
        " $\\vec t^n = 1 : \\vec t^n = (1,0,0,0)$\n",
        "\n",
        " $\\vec t^n = 4 : \\vec t^n = (0,1,0,0)$\n",
        "\n",
        " $\\vec t^n = 5 : \\vec t^n = (0,0,1,0)$\n",
        "\n",
        " $\\vec t^n = 8 : \\vec t^n = (0,0,0,1)$\n",
        "\n",
        " else: $\\vec t^n = (\\frac14,\\frac14,\\frac14,\\frac14)$\n",
        "\n",
        "</center>\n",
        "\n",
        "\n",
        "### Task 1: Target Vectors\n",
        "\n",
        "Implement a function that generates a target vector for any of the ten different classes according to above description. The return value should be a `torch.tensor` of type float."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pKlkqz_ym0L2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "\n",
        "# define the three types of classes\n",
        "known_classes = (1,4,5,8)\n",
        "negative_classes = (0,2,3,7)\n",
        "unknown_classes = (6,9)\n",
        "O = len(known_classes)\n",
        "# define one-hot vectors\n",
        "labels_known = torch.nn.functional.one_hot(torch.arange(0, 4), O)\n",
        "label_unknown = torch.ones(O)*0.25\n",
        "\n",
        "def target_vector(index):\n",
        "  # select correct one-hot vector for known classes, and the 1/O-vectors for unknown classes\n",
        "  if index in known_classes:\n",
        "    return labels_known[known_classes.index(index)]\n",
        "  else:\n",
        "    return label_unknown"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFSme-RNm0L4"
      },
      "source": [
        "### Test 1: Check your Target Vectors\n",
        "\n",
        "Test that your target vectors are correct, for all tpyes of known and unknown samples.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "cn9oEs61m0L4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 tensor([1, 0, 0, 0])\n",
            "4 tensor([0, 1, 0, 0])\n",
            "5 tensor([0, 0, 1, 0])\n",
            "8 tensor([0, 0, 0, 1])\n",
            "0 tensor([0.2500, 0.2500, 0.2500, 0.2500])\n",
            "2 tensor([0.2500, 0.2500, 0.2500, 0.2500])\n",
            "3 tensor([0.2500, 0.2500, 0.2500, 0.2500])\n",
            "7 tensor([0.2500, 0.2500, 0.2500, 0.2500])\n",
            "6 tensor([0.2500, 0.2500, 0.2500, 0.2500])\n",
            "9 tensor([0.2500, 0.2500, 0.2500, 0.2500])\n"
          ]
        }
      ],
      "source": [
        "# check that the target vectors for known classes are correct\n",
        "for index in known_classes:\n",
        "  t = target_vector(index)\n",
        "  print(index, t)\n",
        "  assert max(t) == 1\n",
        "  assert sum(t) == 1\n",
        "\n",
        "# check that the target vectors for unknown classes are correct\n",
        "for index in negative_classes + unknown_classes:\n",
        "  t = target_vector(index)\n",
        "  print(index, t)\n",
        "  assert max(t) == 0.25\n",
        "  assert sum(t) == 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPmC7414m0L5"
      },
      "source": [
        "### Tasks 2 and 3: Dataset Construction and Dataset Item Selection\n",
        "\n",
        "Write a dataset class that derives from `torchvision.datasets.MNIST` in `PyTorch` and adapts some parts of it.\n",
        "In the constructor, make sure that you let `PyTorch` load the dataset by calling the base class constructor `super` with the desired parameters. Afterward, the `self.data` and `self.targets` are populated with all samples and target indexes.\n",
        "From these, we need to sub-select the samples that fit our current `purpose` and store them back to `self.data` and `self.targets`.\n",
        "\n",
        "Second, we need to implement the index function of our dataset, where we need to return both the image and the target vector.\n",
        "The images in `self.data` were originally stored as `uint8` values in the dimension $\\mathbb N^{N\\times28\\times28}$ with values in $[0, 255]$.\n",
        "The targets in `self.targets` were originally stored as class indexes in the dimension $\\mathbb N^N$. Make sure that you return both in the desired format.\n",
        "\n",
        "Notes:\n",
        "\n",
        "* Since Jupyter Notebook does not allow splitting classes over several code boxes, the two tasks are required to be solved in the same code box.\n",
        "* **The definition below is just one possibility.** There are many ways to implement this dataset interface.\n",
        "* With a clever implementation of the constructor, there is no need to overwrite the `__getitem__(self,index)` function.\n",
        "* Depending on your implementation, you might also need to overwrite the `__len__(self)` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "xLuxLw67l4Lu"
      },
      "outputs": [],
      "source": [
        "\n",
        "class DataSet(torchvision.datasets.MNIST):\n",
        "  def __init__(self, purpose=\"train\"):\n",
        "    # call base class constructor to handle the data loading\n",
        "    # make sure that you get the correct part of the data based on the purpose\n",
        "    super(DataSet, self).__init__(\n",
        "      root = './data',\n",
        "      download=True\n",
        "      )\n",
        "    # select the valid classes based on the current purpose\n",
        "    # select the samples that belong to these classes\n",
        "    if purpose != \"test\":\n",
        "      indices = torch.tensor([i for i in range(len(self.targets)) if self.targets[i] in known_classes or self.targets[i] in negative_classes])\n",
        "    else:\n",
        "      indices = torch.tensor([i for i in range(len(self.targets)) if self.targets[i] in known_classes or self.targets[i] in unknown_classes])\n",
        "    # sub-select the data of valid classes\n",
        "    self.data = self.data[indices]\n",
        "    # select the targets of valid classes, and already turn them into target vectors\n",
        "    self.targets = torch.stack([target_vector(self.targets[i]) for i in indices])\n",
        "\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    # perform appropriate actions on the data and the targets\n",
        "    # the format of data should be in [0, 1]\n",
        "    input = self.data[index].float()/255.0  # Normalize data to [0, 1]\n",
        "    input = input.unsqueeze(0)\n",
        "    target = self.targets[index]\n",
        "    return input, target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvmK-kmdm0L6"
      },
      "source": [
        "### Test 2: Data Sets\n",
        "\n",
        "\n",
        "Instantiate the training dataset.\n",
        "Implement a data loader for the training dataset with a batch size of 64.\n",
        "Assure that all inputs are of the desired type and shape.\n",
        "Assert that the target values are in the correct format, and the sum of the target values per sample is one.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "fgFrIjoom0L6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 503: Service Unavailable\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/DataSet/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9912422/9912422 [00:01<00:00, 8146974.04it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/DataSet/raw/train-images-idx3-ubyte.gz to ./data/DataSet/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/DataSet/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 261572.85it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/DataSet/raw/train-labels-idx1-ubyte.gz to ./data/DataSet/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/DataSet/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1648877/1648877 [00:50<00:00, 32859.25it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/DataSet/raw/t10k-images-idx3-ubyte.gz to ./data/DataSet/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/DataSet/raw/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 503: Service Unavailable\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/DataSet/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 958854.88it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/DataSet/raw/t10k-labels-idx1-ubyte.gz to ./data/DataSet/raw\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# instantiate the training dataset\n",
        "\n",
        "train_set = DataSet(purpose=\"train\")\n",
        "train_loader = torch.utils.data.DataLoader(train_set, 64, shuffle=True)\n",
        "# assert that we have not filtered out all samples\n",
        "assert len(train_set)!= 60000 and len(train_set)== 48133\n",
        "\n",
        "# check the batch and assert valid data and sizes\n",
        "for x,t in train_loader:\n",
        "  assert len(x) <= 64\n",
        "  assert len(t) == len(x)\n",
        "  assert torch.all(torch.sum(t, axis = 1) == 1)\n",
        "  assert x.shape == torch.Size([x.shape[0], 1, 28, 28])\n",
        "  assert x.dtype == torch.float32\n",
        "  assert torch.max(x) <= 1\n",
        "  assert torch.min(x) >= 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-7VFbW1IfVl"
      },
      "source": [
        "### Task 4: Data Loader\n",
        "\n",
        "Call the dataset class sperately with batch size of $B=256$, and instantiate data loaders for the three datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "svg_iiEyIfms"
      },
      "outputs": [],
      "source": [
        "batch_size = 256\n",
        "\n",
        "device = 'cpu'\n",
        "\n",
        "# instantiate training dataset and data loader\n",
        "train_set = DataSet(purpose=\"train\")\n",
        "train_loader = torch.utils.data.DataLoader(train_set, 64, shuffle=True)\n",
        "\n",
        "# instantiate validation set and data loader\n",
        "validation_set = DataSet(purpose=\"validation\")\n",
        "validation_loader = torch.utils.data.DataLoader(validation_set, 64, shuffle=True)\n",
        "\n",
        "\n",
        "# instantiate test set and according data loader\n",
        "test_set = DataSet(purpose=\"test\")\n",
        "test_loader = torch.utils.data.DataLoader(test_set, 64, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W67k2w9Mm0L7"
      },
      "source": [
        "### Task 5: Utility Function\n",
        "\n",
        "Implement a function that splits a batch of samples into known and unknown parts. For the known parts, also provide the target vectors.\n",
        "How can we know which of the data samples are known samples, and which are unknown?\n",
        "\n",
        "This function needs to return three elements:\n",
        "* First, the samples from the batch that belong to known classes.\n",
        "* Second, the target vectors that belong to the known classes.\n",
        "* Finally, the samples from the batch that belong to unknown classes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "PSDfc-2Tm0L7"
      },
      "outputs": [],
      "source": [
        "def split_known_unknown(batch, targets):\n",
        "  # select the indexes at which known and unknown samples exist\n",
        "  known = torch.tensor([i for i, target in enumerate(targets) if max(target) == 1])\n",
        "  unknown = torch.tensor([i for i, target in enumerate(targets) if max(target) == 0.25])\n",
        "  # return the known samples, the targets of the known samples, as well as the unknown samples\n",
        "  return batch[known], targets[known], batch[unknown]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVq2US6am0L8"
      },
      "source": [
        "## Loss Function and Confidence\n",
        "\n",
        "We write our own PyTorch implementation of our loss function.\n",
        "Particularly, we implement a manual way to define the derivative of our loss function via `torch.autograd.Function`, which allows us to define the forward and backward pass on our own.\n",
        "For this purpose, we need to implement two `static` functions in our loss.\n",
        "The function `forward(ctx, logits, targets)` is required to compute the loss value and allows us to store some variables in the context of the backward pass.\n",
        "The `backward(ctx, result)` provides us with the result of the forward function (the loss value) as well as the context with our stored variables.\n",
        "Here, we need to compute the derivative of the loss with respect to both of the inputs to the forward function (which might look a bit confusing), i.e.,$\\frac{\\partial \\mathbf{J}^{CCE}}{\\partial \\mathbf{Z}}$ and $\\frac{\\partial \\mathbf{J}^{CCE}}{\\partial \\mathbf{T}}$.\n",
        "Since the latter is not required, we can also simply return `None` for the second derivative.\n",
        "\n",
        "<font color=#FF000>Hint: if you think the implementation of loss function is too hard, you can also cross-entropy as your loss function (**since PyTorch version 1.11**).</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oV8dE2u5B-i"
      },
      "source": [
        "### Task 6: Loss Function Implementation\n",
        "\n",
        "Implement a `torch.autograd.Function` class for the adapted SoftMax function according to the equations provided in the lecture.\n",
        "You might want to compute the log of the network output $\\ln y_o$ from the logits $z_o$ via `torch.nn.functional.log_softmax`.\n",
        "Store all the data required for the backward pass in the context during `forward`, and extract these from the context during `backward`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "czmU2Aqqm0L8"
      },
      "outputs": [],
      "source": [
        "class AdaptedSoftMax(torch.autograd.Function):\n",
        "\n",
        "  # implement the forward propagation\n",
        "  @staticmethod\n",
        "  def forward(ctx, logits, targets):\n",
        "    # compute the log probabilities via log_softmax\n",
        "    y = torch.nn.functional.log_softmax(logits, dim=1)\n",
        "    # save required values for backward pass\n",
        "    ctx.save_for_backward(y, targets)\n",
        "    # compute loss\n",
        "    loss = -torch.sum(targets*y)\n",
        "    return loss\n",
        "\n",
        "  # implement Jacobian\n",
        "  @staticmethod\n",
        "  def backward(ctx, result): \n",
        "    # get results stored from forward pass\n",
        "    y, targets = ctx.saved_tensors\n",
        "    # compute derivative of loss w.r.t. the logits\n",
        "    dJ_dz = y - result#targets\n",
        "    # return the derivatives; none for derivative for the targets\n",
        "    return dJ_dz, None\n",
        "\n",
        "# DO NOT REMOVE!\n",
        "# here we set the adapted softmax function to be used later\n",
        "adapted_softmax = AdaptedSoftMax.apply\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eopp5YC6m0L9"
      },
      "source": [
        "### Task 6a: Alternative Loss Function\n",
        "\n",
        "In case the loss function is too difficult to implement, you can also choose to rely on PyTorch's automatic gradient computation and simply define your loss function without the backward pass.\n",
        "\n",
        "In this case, we only need to define the forward pass. A simple function `adapted_softmax(logits, targets)` is sufficient.\n",
        "\n",
        "You can implement any variant of the categorical cross-entropy loss function on top of SoftMax activations as defined in the lecture.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "fFhw66Pxm0L9"
      },
      "outputs": [],
      "source": [
        "def adapted_softmax_alt(logits, targets):\n",
        "  # compute cross-entropy loss on top of softmax values of the logits\n",
        "  loss = -torch.sum(logits*targets) + 1./targets.shape[0] * torch.logsumexp(logits)\n",
        "  return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDwhq72Vm0L9"
      },
      "source": [
        "### Task 7: Confidence Evaluation\n",
        "\n",
        "Implement a function to compute the confidence value for a given batch of samples. \n",
        "Compute Softmax confidence and split these confidences between known and unknown classes. \n",
        "For samples from known classes, sum up the SoftMax confidences of the correct class. \n",
        "For unknown samples, sum 1 minus the maximum confidence for any of the known classes; also apply the $\\frac1O$ correction for the minimum possible SoftMax confidence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "-ooydMNsm0L9"
      },
      "outputs": [],
      "source": [
        "def confidence(logits, targets):\n",
        "  # compute softmax confidences\n",
        "  y = torch.nn.functional.softmax(logits, dim=1)\n",
        "\n",
        "  # split between known and unknown\n",
        "  known = torch.tensor([i for i in range(len(targets)) if max(targets[i]) == 1], dtype=torch.long)\n",
        "  unknown = torch.tensor([i for i in range(len(targets)) if max(targets[i]) == 0.25], dtype=torch.long)\n",
        "  conf_known = torch.sum(y[known])\n",
        "  conf_unknown = 1-torch.max(y[unknown], dim=1).values + 1./targets.shape[0]\n",
        "  return conf_known + conf_unknown"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e543YdEvm0L-"
      },
      "source": [
        "### Test 3: Check Confidence Implementation\n",
        "\n",
        "Test that your confidence implementation does what it is supposed to do.\n",
        "\n",
        "Note that confidence values should always be between 0 and 1, other values indicate an issue in the implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "TXAruuB0m0L-"
      },
      "outputs": [],
      "source": [
        "# select good logit vectors for known and unknown classes\n",
        "logits = torch.tensor([[10., 0., 0., 0.], [-10., 0, -10., -10.], [0.,0.,0.,0.]])\n",
        "# select the according target vectors for these classes\n",
        "targets = torch.stack([target_vector(known_classes[0]), target_vector(known_classes[1]), target_vector(negative_classes[0])])\n",
        "# the confidence should be close to 1 for all cases\n",
        "assert 3 - confidence(logits, targets) < 1e-3\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWFgh2Nwm0L-"
      },
      "source": [
        "## Network and Training\n",
        "\n",
        "We make use of the same convolutional network as utilized in Assignment 6, to which we append a final fully-connected layer with $K$ inputs and $O$ outputs.\n",
        "Additionally, we replace the $\\sigma$ activation function with the better-performing **PReLU** function.\n",
        "\n",
        "The topology can be found in the following:\n",
        "1. 2D convolutional layer with $Q_1$ channels, kernel size $7\\times7$, stride 1 and padding 0\n",
        "2. 2D maximum pooling layer with kernel size $2\\times2$ and stride 2\n",
        "3. activation function **PReLU**\n",
        "4. 2D convolutional layer with $Q_2$ channels, kernel size $5\\times5$, stride 1 and padding 2\n",
        "5. 2D maximum pooling layer with kernel size $2\\times2$ and stride 2\n",
        "6. activation function **PReLU**\n",
        "7. flatten layer to convert the convolution output into a vector\n",
        "8. fully-connected layer with the correct number of inputs and $K$ outputs\n",
        "9. fully-connected layer with $K$ inputs and $O$ outputs\n",
        "\n",
        "\n",
        "\n",
        "However, instead of relying on the `torch.nn.Sequential` class, we need to define our own network class, which we need to derive from `torch.nn.Module` -- since our network has two outputs.\n",
        "We basically need to implement two methods in our network.\n",
        "The constructor `__init__(self, Q1, Q2, K)` needs to call the base class constructor and initialize all required layers of our network.\n",
        "The `forward(self, x)` function then passes the input through all of our layers and returns both the deep features (extracted at the first fully-connected layer) and the logits (extracted from the second fully-connected layer)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAo2Qx9q5bk8"
      },
      "source": [
        "\n",
        "### Task 8: Network Definition\n",
        "\n",
        "We define our own small-scale network to classify known and unknown samples for MNIST.\n",
        "\n",
        "However, this time we need to implement our own network model since we need to modify our network output.\n",
        "\n",
        "Implement a network class, including the layers as provided above.\n",
        "Implement both the constructor and the forward function.\n",
        "Instantiate the network with $Q_1=32, Q_2=32, K=20, O=4$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "LYyPEmsWm0L-"
      },
      "outputs": [],
      "source": [
        "class Network (torch.nn.Module):\n",
        "  def __init__(self, Q1=32, Q2=32, K=20, O=4):\n",
        "    # call base class constrcutor\n",
        "    super(Network,self).__init__()\n",
        "    # define convolutional layers\n",
        "    self.conv1 = torch.nn.Conv2d(1, Q1, kernel_size = 7, stride = 1, padding=0)\n",
        "    self.conv2 = torch.nn.Conv2d(Q1, Q2, kernel_size = 5, stride = 1, padding=2)\n",
        "    # pooling and activation functions will be re-used for the different stages\n",
        "    self.pool = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "    self.act = torch.nn.PReLU()\n",
        "    # define fully-connected layers\n",
        "    self.flatten = torch.nn.Flatten()\n",
        "    self.fc1 = torch.nn.Linear(Q2*5*5, K) #TODO:check the size of the input\n",
        "    self.fc2 = torch.nn.Linear(K, O)\n",
        "\n",
        "  def forward(self,x):\n",
        "    # compute first layer of convolution, pooling and activation\n",
        "    a = self.act(self.pool(self.conv1(x)))\n",
        "    # compute second layer of convolution, pooling and activation\n",
        "    a = self.act(self.pool(self.conv2(a)))\n",
        "    # get the deep features as the output of the first fully-connected layer\n",
        "    deep_features = self.fc1(self.flatten(a))\n",
        "    # get the logits as the output of the second fully-connected layer\n",
        "    logits = self.fc2(deep_features)\n",
        "    # return both the logits and the deep features\n",
        "    return logits, deep_features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rxnrxtRm0L-"
      },
      "source": [
        "### Task 9: Training Loop\n",
        "\n",
        "Implement a function for training network, which contains the training and validation loop. \n",
        "Compute the training set confidence during the epoch.\n",
        "At the end of each epoch, also compute the validation set confidence measure.\n",
        "Print both the training set and validation set confidence scores to the console. \n",
        "Finally, return the trained network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "wnLIrcTzm0L_"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "def train(network,epochs, eta, momentum, loss_function):\n",
        "\n",
        "  # Set GPU\n",
        "  network = network.to(device)\n",
        "\n",
        "  # SGD optimizer with appropriate learning rate\n",
        "  optimizer = torch.optim.SGD(network.parameters(), lr=eta, momentum=momentum)\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    # evaluate average confidence for training and validation set\n",
        "    train_conf = validation_conf = 0.\n",
        "\n",
        "    for x,t in tqdm(train_loader):\n",
        "      # extract logits (and deep features) from network\n",
        "      logits, deep_features = network(x.to(device))\n",
        "      # compute loss\n",
        "      loss = loss_function(logits, t.to(device))\n",
        "      # perform weight update\n",
        "      optimizer.zero_grad()\n",
        "      # loss.backward(loss)\n",
        "      optimizer.step()\n",
        "      # compute training confidence\n",
        "      train_conf += torch.mean(confidence(logits, t))\n",
        "\n",
        "    # compute validation comfidence\n",
        "    with torch.no_grad():\n",
        "      for x,t in tqdm(validation_loader):\n",
        "        # extract logits (and deep features)\n",
        "        logits, deep_features = network(x.to(device))\n",
        "        # compute validation confidence\n",
        "        validation_conf += torch.mean(confidence(logits, t))\n",
        "    # print average confidence for training and validation\n",
        "    print(f\"\\rEpoch {epoch}; train: {train_conf/len(train_set):1.5f}, val: {validation_conf/len(validation_set):1.5f}\")\n",
        "\n",
        "  return network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MVLbae8RErZ"
      },
      "source": [
        "### Task 10: Network training\n",
        "\n",
        "Instantiate network with $K1=K2=32$, $K=20$ and $O = 4$. Train the network for 30 epochs with an appropriate learning rate (the optimal learning rate might depend on your loss function implementation and can vary between 0.1 and 0.00001), momentum=$0.9$, and call the function you defined in Task 9."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "JiZKtxFdHaJx"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 753/753 [00:21<00:00, 34.59it/s]\n",
            "100%|██████████| 753/753 [00:14<00:00, 51.20it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0; train: 0.50678, val: 0.50678\n"
          ]
        }
      ],
      "source": [
        "loss_function = adapted_softmax\n",
        "network_adapted = Network()\n",
        "network_adapted = train(network_adapted, 1, 0.01, 0.9, loss_function)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDqRH6Bsm0L_"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "For evaluation, we test two different things.\n",
        "First, we check whether our intuition is correct, and the training helps reduce the deep feature magnitudes of unknown samples while maintaining magnitudes for known samples.\n",
        "It is also interesting to see whether there is a difference between samples of the negative classes that were seen during training, and unknown classes that were not.\n",
        "For this purpose, we extract the deep features for the validation and test sets, compute their magnitudes, and plot them in a histogram.\n",
        "\n",
        "The second evaluation computes Correct Classification Rates (CCR) and False Positive Rates (FPR) for a given confidence threshold $\\zeta=0.98$ (based on your training results, you might want to vary this threshold).\n",
        "For the known samples, we compute how often the correct class was classified with a confidence over threshold.\n",
        "For unknown samples, we assess how often one of the known classes was predicted with a confidence larger than $\\zeta$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnwwGYj17g2J"
      },
      "source": [
        "### Task 11: Feature Magnitude Plot\n",
        "\n",
        "Define a function, which can extract deep features for validation and test set samples and compute their magnitudes. Split them into known, negative (validation set), and unknown (test set). Plot a histogram for each of the three types of samples.\n",
        "Note that the minimum magnitude is 0, and the maximum magnitude can depend on your network training success."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "iM0SayyKm0L_"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 288x144 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "def plot_features(network):\n",
        "  # collect feature magnitudes for\n",
        "  known, negative, unknown = [], [], []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    # extract deep features magnitudes for validation set\n",
        "    for x,t in validation_loader:\n",
        "      # extract deep features (and logits)\n",
        "      logits, deep_features = network(x)\n",
        "      # compute norms\n",
        "      norms = torch.norm(deep_features, dim=1)\n",
        "      # split between known and unknown\n",
        "      known_batch, known_targets, unknown_batch = split_known_unknown(norms, t)\n",
        "      # collect norms of known samples\n",
        "      known.extend(known_batch)\n",
        "      # collect norms of negative samples\n",
        "      negative.extend([known_batch[i] for i in range(len(known_batch)) if max(known_targets[i]) == 0.25])\n",
        "\n",
        "    for x,t in test_loader:\n",
        "      # extract deep features (and logits)\n",
        "      logits, deep_features = network(x)\n",
        "      # compute norms\n",
        "      norms = torch.norm(deep_features, dim=1)\n",
        "      # split between known and unknown\n",
        "      known_batch, known_targets, unknown_batch = split_known_unknown(norms, t)\n",
        "      # collect norms of known samples\n",
        "      known.extend(known_batch)\n",
        "      # collect norms of unknown samples\n",
        "      unknown.extend(unknown_batch)\n",
        "\n",
        "\n",
        "  # plot the norms as histograms\n",
        "  from matplotlib import pyplot\n",
        "  pyplot.figure(figsize=(4,2))\n",
        "\n",
        "  # keep the same maximum magnitude\n",
        "  max_mag = 20\n",
        "  # plot the three histograms\n",
        "  pyplot.hist(known, bins=100, range=(0,max_mag), density=True, color=\"g\", histtype=\"step\", label=\"Known\")\n",
        "  pyplot.hist(negative, bins=100, range=(0,max_mag), density=True, color=\"b\", histtype=\"step\", label=\"Negative\")\n",
        "  pyplot.hist(unknown, bins=100, range=(0,max_mag), density=True, color=\"r\", histtype=\"step\", label=\"Unknown\")\n",
        "\n",
        "  # beautify plot\n",
        "  pyplot.legend()\n",
        "  pyplot.xlabel(\"Deep Feature Magnitude\")\n",
        "  pyplot.ylabel(\"Density\")\n",
        "\n",
        "\n",
        "plot_features(network_adapted)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTSMfeJ_m0L_"
      },
      "source": [
        "### Task 12: Classification Evaluation\n",
        "\n",
        "For a fixed threshold of $\\zeta=0.98$, compute CCR and FPR for the test set.\n",
        "A well-trained network can achieve a CCR of > 90% for an FPR < 10%.\n",
        "You might need to vary the threshold."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QlrBZlHxm0L_"
      },
      "outputs": [],
      "source": [
        "def evaluation(network):\n",
        "  zeta = 0.98\n",
        "\n",
        "  # count the correctly classified and the total number of known samples\n",
        "  correct = known = 0\n",
        "  # count the incorrectly classified and the total number of unknown samples\n",
        "  false = unknown = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for x,t in test_loader:\n",
        "      # extract logits (and deep features)\n",
        "      ...\n",
        "      # compute softmax confidences\n",
        "      ...\n",
        "      # split between known and unknown\n",
        "      ...\n",
        "\n",
        "      # compute number of correctly classified knowns above threshold\n",
        "      correct += ...\n",
        "      known += ...\n",
        "\n",
        "      # compute number of incorrectly accepted known samples\n",
        "      false += ...\n",
        "      unknown += ...\n",
        "\n",
        "  # print both rates\n",
        "  print (f\"CCR: {correct} of {known} = {correct/known*100:2.2f}%\")\n",
        "  print (f\"FPR: {false} of {unknown} = {false/unknown*100:2.2f}%\")\n",
        "\n",
        "\n",
        "evaluation(...)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "PN2b5uP2GOPh",
        "CNrxHSLz4p3y",
        "eRwI_rHe42nA"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "interpreter": {
      "hash": "2dd53f8ad749bca69f7250ce75eb4f0def59db5cf79075a9716322ffc58e8a2e"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
